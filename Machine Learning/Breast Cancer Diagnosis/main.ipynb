{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'oral cancer.v3i.tensorflow'\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'valid')\n",
    "\n",
    "test_annotations = pd.read_csv(os.path.join(TEST_DIR, '_annotations.csv'))\n",
    "train_annotations = pd.read_csv(os.path.join(TRAIN_DIR, '_annotations.csv'))\n",
    "valid_annotations = pd.read_csv(os.path.join(VALID_DIR, '_annotations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations(annotations, img_dir):\n",
    "    \"\"\"\n",
    "    Filters the annotations to include only images present in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        annotations (pandas.DataFrame): The annotations DataFrame.\n",
    "        img_dir (str): The directory path containing the images.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The filtered annotations DataFrame.\n",
    "    \"\"\"\n",
    "    image_files = [f for f in os.listdir(img_dir)]\n",
    "    annotations = annotations[annotations['filename'].isin(image_files)]\n",
    "    annotations = annotations.drop_duplicates(subset=['filename'])\n",
    "    return annotations\n",
    "\n",
    "test_annotations = filter_annotations(test_annotations, os.path.join(TEST_DIR))\n",
    "train_annotations = filter_annotations(train_annotations, os.path.join(TRAIN_DIR))\n",
    "valid_annotations = filter_annotations(valid_annotations, os.path.join(VALID_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "6411\n",
      "603\n"
     ]
    }
   ],
   "source": [
    "print(len(test_annotations))\n",
    "print(len(train_annotations))\n",
    "print(len(valid_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 220\n",
      "3226 3185\n",
      "295 308\n"
     ]
    }
   ],
   "source": [
    "def separate_images(annotations):\n",
    "    \"\"\"\n",
    "    Separates the images into cancerous and non-cancerous groups based on the annotations.\n",
    "\n",
    "    Args:\n",
    "        annotations (pandas.DataFrame): The annotations DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            cancerous (list): A list of cancerous image paths.\n",
    "            non_cancerous (list): A list of non-cancerous image paths.\n",
    "    \"\"\"\n",
    "    cancerous = []\n",
    "    non_cancerous = []\n",
    "\n",
    "    for _, row in annotations.iterrows():\n",
    "        image_path = os.path.join(row['filename'])\n",
    "        if row['class'] == \"cancer\":  # Cancerous\n",
    "            cancerous.append(image_path)\n",
    "        elif row[\"class\"] == \"no cancer\":  # Non-cancerous\n",
    "            non_cancerous.append(image_path)\n",
    "\n",
    "    return cancerous, non_cancerous\n",
    "\n",
    "test_cancerous, test_non_cancerous = separate_images(test_annotations)\n",
    "train_cancerous, train_non_cancerous = separate_images(train_annotations)\n",
    "valid_cancerous, valid_non_cancerous = separate_images(valid_annotations)\n",
    "print(len(test_cancerous), len(test_non_cancerous))\n",
    "print(len(train_cancerous), len(train_non_cancerous))\n",
    "print(len(valid_cancerous), len(valid_non_cancerous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancerous_image_names = test_cancerous[:100]# + valid_cancerous + train_cancerous\n",
    "non_cancerous_image_names = test_non_cancerous[:100]# + valid_non_cancerous + train_non_cancerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancerous_image_paths = [os.path.join(TEST_DIR, img) for img in test_cancerous] + [os.path.join(VALID_DIR, img) for img in valid_cancerous]# + [os.path.join(TRAIN_DIR, img) for img in train_cancerous] \n",
    "non_cancerous_image_paths = [os.path.join(TEST_DIR, img) for img in test_non_cancerous] + [os.path.join(VALID_DIR, img) for img in valid_non_cancerous]# + [os.path.join(TRAIN_DIR, img) for img in train_non_cancerous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import exposure, filters, restoration, transform, util\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# Helper function to load and preprocess an image\n",
    "def load_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize and pad to maintain aspect ratio\n",
    "    image = np.array(image)\n",
    "    max_side = max(image.shape[:2])\n",
    "    delta = max_side - min(image.shape[:2])\n",
    "    top, bottom = delta // 2, delta - (delta // 2)\n",
    "    left, right = 0, 0\n",
    "    if image.shape[0] < image.shape[1]:\n",
    "        left, right = delta // 2, delta - (delta // 2)\n",
    "    image = np.pad(image, [(top, bottom), (left, right), (0, 0)], mode='constant')\n",
    "    image = transform.resize(image, target_size, anti_aliasing=True)\n",
    "\n",
    "    # Data augmentation: Random rotations and flips\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = np.flip(image, axis=0)  # Vertical flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = np.flip(image, axis=1)  # Horizontal flip\n",
    "    angle = np.random.randint(-20, 20)\n",
    "    image = transform.rotate(image, angle, preserve_range=True)\n",
    "\n",
    "    # Normalize using mean and std\n",
    "    image = (image - np.mean(image)) / np.std(image)\n",
    "\n",
    "    # Clip pixel values to the valid range\n",
    "    image = np.clip(image, -1.0, 1.0)\n",
    "\n",
    "    # Contrast enhancement using CLAHE\n",
    "    image = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "\n",
    "    # Denoising using Non-Local Means\n",
    "    sigma_est = np.mean(estimate_sigma(image, channel_axis=-1))\n",
    "    patch_kw = dict(patch_size=5, patch_distance=6, channel_axis=-1)\n",
    "    image = denoise_nl_means(image, h=0.6 * sigma_est, sigma=sigma_est, fast_mode=True, **patch_kw)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Load and preprocess all images\n",
    "cancerous_images = [load_preprocess_image(path) for path in cancerous_image_paths]\n",
    "non_cancerous_images = [load_preprocess_image(path) for path in non_cancerous_image_paths]\n",
    "\n",
    "# Combine images and labels\n",
    "X = cancerous_images + non_cancerous_images\n",
    "y = [1] * len(cancerous_images) + [0] * len(non_cancerous_images)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "# Load the pre-trained EfficientNetB7 model without the top classification layers\n",
    "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "\n",
    "\n",
    "# Augment training data\n",
    "train_generator = data_generator.flow(np.array(X_train), np.array(y_train), batch_size=32)# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=(np.array(X_test), np.array(y_test)),\n",
    "    callbacks=[early_stop, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">64,097,687</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,232</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb7 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2560\u001b[0m)     │    \u001b[38;5;34m64,097,687\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,311,232\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,032,924</span> (259.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,032,924\u001b[0m (259.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,745</span> (5.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,311,745\u001b[0m (5.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,097,687</span> (244.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m64,097,687\u001b[0m (244.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,623,492</span> (10.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,623,492\u001b[0m (10.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 7s/step\n",
      "Accuracy: 45.63%\n",
      "Precision: 45.90%\n",
      "Recall: 54.90%\n",
      "F1-score: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "import time \n",
    "\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "best_model.summary()\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = (best_model.predict(np.array(X_test)) > 0.5).astype(int).ravel()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1-score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(image_paths, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extracts features from a list of image paths using a pre-trained CNN.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): A list of paths to the input images.\n",
    "        batch_size (int): The batch size for feature extraction.\n",
    "        \n",
    "    Returns:\n",
    "        features (torch.Tensor): A tensor containing the extracted features for all images.\n",
    "    \"\"\"\n",
    "    # Define data preprocessing and augmentation steps\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the pre-trained ResNet-50 model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a feature extractor by removing the final classification layer\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    # Initialize lists to store features and image tensors\n",
    "    features = []\n",
    "    image_tensors = []\n",
    "    \n",
    "    # Loop over the image paths and extract features\n",
    "    for path in image_paths:\n",
    "        image = Image.open(path)\n",
    "        image_tensor = data_transforms(image).unsqueeze(0)\n",
    "        image_tensors.append(image_tensor)\n",
    "        \n",
    "        # Extract features in batches for efficiency\n",
    "        if len(image_tensors) == batch_size:\n",
    "            batch_tensor = torch.cat(image_tensors, dim=0)\n",
    "            batch_features = feature_extractor(batch_tensor)\n",
    "            features.append(batch_features.squeeze())\n",
    "            image_tensors = []\n",
    "    \n",
    "    # Extract features for the remaining images\n",
    "    if image_tensors:\n",
    "        batch_tensor = torch.cat(image_tensors, dim=0)\n",
    "        batch_features = feature_extractor(batch_tensor)\n",
    "        features.append(batch_features.squeeze())\n",
    "    \n",
    "    # Concatenate all features into a single tensor\n",
    "    features = torch.cat(features, dim=0)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from cancerous and non-cancerous images\n",
    "cancerous_features = extract_features(cancerous_image_paths)\n",
    "non_cancerous_features = extract_features(non_cancerous_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels\n",
    "all_features = torch.cat([cancerous_features, non_cancerous_features], dim=0)\n",
    "all_labels = torch.tensor([1] * len(cancerous_image_paths) + [0] * len(non_cancerous_image_paths))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "# Define data augmentation and preprocessing steps\n",
    "data_transforms = transforms.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=20),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply data augmentation to training data\n",
    "X_train = [data_transforms(img.view(1, 1, 2048)) for img in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM classifier\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
